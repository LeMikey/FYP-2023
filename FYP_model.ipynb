{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrpQbX3SpPzCvHr7mPJZ0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeMikey/FYP-2023/blob/main/FYP_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFD9DuKcAQ1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.functional import to_map_style_dataset, sentencepiece_tokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from functools import partial\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Filter only English tweets\n",
        "# Convert all columns to string type\n",
        "# Drop all rows with any NaN and NaT values\n",
        "UnC_Data = pd.read_csv('./Dataset/Text/4/dataset.csv', low_memory=False).query('Language == \"en\"').astype(str).dropna()\n",
        "\n",
        "# Only keep the text and label columns\n",
        "UnC_Data = UnC_Data[['Text', 'Label']]\n",
        "\n",
        "# Rename the \"Label\" column as \"sentiment labels\"\n",
        "UnC_Data = UnC_Data.rename(columns={'Label': 'Sentiment_Labels'})\n",
        "\n",
        "# Visualize the distribution of the labels\n",
        "UnC_Data.groupby(['Sentiment_Labels']).size().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to remove emojis\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                       u\"\\U0001F600-\\U0001F64F\"\n",
        "                       u\"\\U0001F300-\\U0001F5FF\"\n",
        "                       u\"\\U0001F680-\\U0001F6FF\"\n",
        "                       u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                       u\"\\U00002702-\\U000027B0\"\n",
        "                       u\"\\U000024C2-\\U0001F251\"\n",
        "                       \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "# Function to remove unwanted characters and symbols\n",
        "def clean_tweet(tweet):\n",
        "    if isinstance(tweet, float):\n",
        "        return \"\"\n",
        "    temp = tweet.lower()\n",
        "    temp = re.sub(\"'\", \"\", temp)\n",
        "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
        "    temp = re.sub(\"#\",\"\", temp)\n",
        "    temp = remove_emoji(temp)\n",
        "    temp = re.sub(r'http\\S+', '', temp)\n",
        "    temp = re.sub('[()!?]', ' ', temp)\n",
        "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
        "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
        "    return temp\n",
        "\n",
        "# Function to preprocess the dataset with tokenization, lemmatization, and stop words removal using spaCy\n",
        "def preprocess_tweet_spacy(tweet):\n",
        "    doc = nlp(tweet)\n",
        "    filtered_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply the cleaning function to the 'Text' column using vectorized operations\n",
        "UnC_Data['Text'] = UnC_Data['Text'].apply(clean_tweet)\n",
        "\n",
        "# Use spaCy for tokenization, lemmatization, and stop words removal\n",
        "UnC_Data['Text'] = UnC_Data['Text'].apply(lambda x: preprocess_tweet_spacy(x))"
      ],
      "metadata": {
        "id": "BxgBk2GOA6Yh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}